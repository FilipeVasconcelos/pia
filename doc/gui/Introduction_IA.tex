% Created 2017-11-03 ven. 15:27
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage[frenchb]{babel}
\author{F. Vasconcelos, G. Roux}
\date{\today}
\title{Introduction à l'intelligence artificielle}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 24.4.1 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\tableofcontents



\section{Introduction}
\label{sec-1}

\section{Le OU exclusif}
\label{sec-2}

$W^{1}=\begin{pmatrix}0\\0\end{pmatrix}$

\section{Prérequis mathématiques et equivalant en Python}
\label{sec-3}

Le package Numpy est indispensable pour manipuler convenablement les vecteurs
et les matrices.

\subsection{Déclaration d'un vecteur, d'une matrice}
\label{sec-3-1}

\begin{verbatim}
import numpy as np
X = np.array([[0,0],[0,1],[1,0],[1,1]])
print(X)
\end{verbatim}

Ce qui produit la sortie:

Observons tout de suite la syntaxe pour transposer une matrice:

\begin{verbatim}
print(X.T)
\end{verbatim}

Ce qui produit la sortie:

\subsection{Le produit matriciel grâce à la fonction dot:}
\label{sec-3-2}

\begin{verbatim}
L = np.array([ [10, 20, 30, 40] ])
C = np.array([ [2, 4, 6, 8] ]).T
print("L =", L)
print("C =", C)
print("L.C =", np.dot(L, C))
print("C.L =", np.dot(C, L))
\end{verbatim}

Sortie:

\subsection{Génération aléatoire de matrices}
\label{sec-3-3}

\begin{verbatim}
A = np.random.uniform(size=(2, 5))
B = np.random.uniform(size=(3, 1))

print(A)
print(B)
\end{verbatim}

Sortie:

\subsection{Produit de Hadamard}
\label{sec-3-4}

\textbf{Définition}

Soient $A=\begin{pmatrix}a_{ij}\end{pmatrix}$ et
$B=\begin{pmatrix}b_{ij}\end{pmatrix}$ deux matrices de même dimension. L
produit de Hadamard de $A$ et de $B$, noté $A\times B$ est défini par:

\begin{equation}
A\times B := \begin{pmatrix}a_{ij}b_{ij}\end{pmatrix}
\end{equation}

\begin{verbatim}
A = np.cumsum(np.ones((2, 5)), axis = 1)
B = 2 * np.ones((1, 5))

print(A)
print(B)

print(A * B)
\end{verbatim}

Sortie:


\section{Un premier exemple: un réseau de neurones de type "feed forward"}
\label{sec-4}

\subsection{Terminologie}
\label{sec-4-1}

\textbf{Notation}

Soit $\sigma$ une fonction définie sur $\mathbb{R}$ et
$M=\begin{pmatrix}m_{ij}
\end{pmatrix}_{\substack{1\leq i\leq n\\1\leq j\leq p }}$ une matrice à
coefficients réels. On notera $\sigma(M)$ la matrice définie par:

\begin{equation}
\sigma(M):=\begin{pmatrix}
\sigma(m_{ij})
\end{pmatrix}_{\substack{1\leq i\leq n\\1\leq j\leq p }}
\end{equation}

\textbf{Définition}

Soit $N$ un entier naturel. On appelle réseau de neurones à $N$ couches toute
suite finie de couples $(W^{(1)},\sigma^{(1)})$, $(W^{(2)},\sigma^{(2)})$, \ldots{},
$(W^{(N)},\sigma^{(N)})$, telle que:

\begin{enumerate}
\item $W^{(n)}\in\mathcal{M}_{m_{j+1}m_{j}}(\mathbb(R)$, où $m_{0}$, \ldots{}, $m_N$ est
une suite d'entiers positifs, où $\mathcal{M}_{np}(\mathbb(R))$ désigne
l'ensemble des matrices à coefficients réels à $n$ lignes et $p$ colonnes.
\item $\sigma^{(n)}$ est une fonction définie (et dérivable) sur $\mathbb{R}$
\end{enumerate}

$n_{i}$ est la taille de la $i^{eme}$ couche du réseau.

L'entrée du réseau est la colonne notée
$X^{(0)}\in\mathcal{M}_{n_{0},1}(\mathbb{R})$. On définit par récurrence la
suite $X^{(1)}$, \ldots{}, $X^{(N)}$ des entrées des neurones ainsi que la suite
$Y^{(0)}$, \ldots{}, $Y^{(N)}$ des sorties des neurones par:

\begin{equation}
\begin{cases}
Y^{(0)}=X^{(0)}\\
\forall n\in \left\{1,...,N\right\}, X^{(n)}=W^{(n)}Y^{(n-1)}\\
\forall n\in \left\{1,...,N\right\}, Y^{(n)}=\sigma^{n}(X^{(n)})
\end{cases}
\end{equation}

Si $X^{(0)}$ est l'entrée du réseau de neurones, on appelle sortie la matrice la
matrice $Y^{(N)}$.

\subsection{Expression de l'erreur}
\label{sec-4-2}

\textbf{Définition}

Notons $T\in\mathcal{M}_{m_{N},1}(\mathbb{R})$ la sortie théorique du réseau. On
appelle erreur commise par le réseau, et on la note $E$, le nombre défini par:

\begin{equation}
E:=\frac{1}{2}\sum_{i=1}^{m_{N}}(T_{i}-Y_{i}^{(N)})^{2}
\end{equation}

\textbf{Remarque:}

$Y^{(N)}$ est une fonction (dérivable si les $\sigma^{(n)}$ le sont) des
$m_{1}\times m_{0}+\dots+m_{N}\times m_{N-1}$ variables
$\left\{w_{ij}^{(n)}\right\}$.

\subsection{Calcul des dérivées partielles de l'erreur}
\label{sec-4-3}

\subsubsection{Par rapport aux coefficients de la dernière matrice}
\label{sec-4-3-1}

On calcule ici les dérivées partielles du type
$\frac{\partial E}{\partial w_{ij}^{(N)}}$.

\begin{align}
\frac{\partial E}{\partial w_{ij}^{(N)}}
& = \frac{\partial \frac{1}{2}\sum_{k=1}^{m_{N}}
(t_{k}-y_{k}^{(N)})^{2}}{\partial w_{ij}^{(N)}}\\
& = \frac{1}{2}\sum_{i=k}^{m_{N}}
\frac{\partial (t_{k}-y_{k}^{(k)})^{2}}{\partial w_{ij}^{(N)}}\\
& = \sum_{k=1}^{m_{N}}(t_{k}-t_{k}^{(N)})
\frac{\partial (t_{k}-y_{k}^{(N)})}{\partial w_{ij}^{(N)}}\\
& = -\sum_{k=1}^{m_{N}}(t_{k}-y_{k}^{(N)})
\frac{\partial (\sigma^{(N)}(x_{k}^{(N)}))}{\partial w_{ij}^{(N)}}\\
& = -(t_{i}-y_{i}^{(N)})
\frac{\partial (\sigma^{(N)}(x_{i}^{(N)}))}{\partial w_{ij}^{(N)}}\\
& = -(t_{i}-y_{i}^{(N)})\sigma^{(N)}'(x_{i}^{(N)})
\frac{\partial (x_{i}^{(N)})}{\partial w_{ij}^{(N)}}\\
& = -(t_{i}-y_{i}^{(N)})\sigma^{(N)}'(x_{i}^{(N)})
\frac{\partial (\sum_{k=1}^{n_{N-1}}w_{ik}^{(N)}y_{k}^{(N-1)})}
{\partial w_{ij}^{(N)}}\\
& = -(t_{i}-y_{i}^{(N)})\sigma^{(N)}'(x_{i}^{(N)})y_{j}^{(N-1)}\\
\end{align}

\textbf{Remarques:}

La règle de dérivation des fonctions composées donne:

\begin{align}
\frac{\partial E}{\partial w_{ij}^{(N)}}
& = \sum_{k=1}^{m_{N}}\frac{\partial E}{\partial x_{k}^{(N)}}
\frac{\partial x_{k}^{(N)}}{\partial w_{ij}^{(N)}}\\
& = \sum_{k=1}^{m_{N}}\frac{\partial E}{\partial x_{k}^{(N)}}
\frac{\partial (\sum_{k=1}^{n_{N-1}}w_{ik}^{(N)}y_{k}^{(N-1)})}
{\partial w_{ij}^{(N)}}\\
& = \frac{\partial E}{\partial x_{i}^{(N)}}y_{j}^{(N-1)}
\end{align}

Cela pousse à definir la grandeur, appelée signal d'erreur du neurone $i$ de la
$n^{eme}$ couche, le nombre:

\begin{equation}
\delta_{i}^{(n)}=-\frac{\partial E}{\partial x_{i}^{(n)}}
\end{equation}

Caculons $\delta_{i}^{(N)}$:

\begin{align}
\delta_{i}^{(N)}
& = -\frac{\partial E}{\partial x_{i}^{(N)}}\\
& = +\sum_{k=1}^{m_{N}}(t_{k}-y_{k}^{(N)})
\frac{\partial (\sigma^{(N)}(x_{k}^{(N)}))}{\partial x_{i}^{(N)}}\\
& = +(t_{i}-y_{i}^{(N)})\sigma^{(N)}'(x_{i}^{(N)})
\end{align}

Cela permet la notation abrégée et généralisable suivante:

\begin{equation}
\frac{\partial E}{\partial w_{ij}^{(N)}}
= -\delta_{i}^{(N)}y_{j}^{(N-1)}
\end{equation}

\subsubsection{Par rapport aux coefficients des matrices des couches cachées}
\label{sec-4-3-2}

Calculons $\delta_{i}^{n}$ pour $n<N$:

\begin{align}
\delta_{i}^{(n)}
& = -\frac{\partial E}{\partial x_{i}^{(n)}}\\
& = +\sum_{k=1}^{m_{N}}(t_{k}-y_{k}^{(n)})
\frac{\partial (\sigma^{(N)}(x_{k}^{(N)}))}{\partial x_{i}^{n}}\\
& = +\sum_{k=1}^{m_{N}}(t_{k}-y_{k}^{(n)})
\sum_{l=1}^{m_{n+1}}
\frac{\partial \left(\sigma^{(N)}(x_{l}^{(N)})\right)}
{\partial x_{l}^{n+1}}
\frac{\partial x_{l}^{n+1}}{\partial x_{i}^{(n)}}\\
& = \sum_{l=1}^{m_{n+1}}\left(\sum_{k=1}^{m_{N}}(t_{k}-y_{k}^{(n)})
\frac{\partial \left(\sigma^{(N)}(x_{l}^{(N)})\right)}
{\partial x_{l}^{(n+1)}}\right)
\frac{\partial x_{l}^{(n+1)}}{\partial x_{i}^{(n)}}\\
& = \sum_{l=1}^{m_{n+1}}\delta_{l}^{(n+1)}
\frac{\partial x_{l}^{(n+1)}}{\partial x_{i}^{(n)}}\\
& = \sum_{l=1}^{m_{n+1}}\delta_{l}^{(n+1)}
\frac{\partial \left(\sum_{k=1}^{m_{n}}w_{lk}^{(n+1)}y_{k}^{(n)}
\right)}{\partial x_{i}^{(n)}}\\
& = \sum_{l=1}^{m_{n+1}}\delta_{l}^{(n+1)}
\frac{\partial \left(\sum_{k=1}^{m_{n}}w_{lk}^{(n+1)}
\sigma^{(n)}(x_{k}^{(n)})
\right)}{\partial x_{i}^{(n)}}\\
& = \sum_{l=1}^{m_{n+1}}w_{li}^{n+1}\delta_{l}^{(n+1)}\sigma^{(n)}'(x_{i}^{(n)})\\
& = \sigma^{(n)}'(x_{i}^{(n)})\sum_{l=1}^{m_{n+1}}w_{li}^{n+1}\delta_{l}^{(n+1)}
\end{align}

La relation ci-dessus est très importante dans la propagation de l'erreur.

Calculons $\frac{\partial E}{\partial w_{ij}^{(n)}}$:

\begin{align}
\frac{\partial E}{\partial w_{ij}^{(n)}}
& = \sum_{k=1}^{m_{N}}\frac{\partial E}{\partial x_{k}^{(n)}}
\frac{\partial x_{k}^{(n)}}{\partial w_{ij}^{(n)}}\\
& = \sum_{k=1}^{m_{N}}\frac{\partial E}{\partial x_{k}^{(n)}}
\frac{\partial (\sum_{k=1}^{n_{n-1}}w_{ik}^{(n)}y_{k}^{(n-1)})}
{\partial w_{ij}^{(n)}}\\
& = \frac{\partial E}{\partial x_{i}^{(n)}}y_{j}^{(n-1)}\\
& = -\delta_{i}^{(n)}y_{j}^{(n-1)}
\end{align}


\section{Optimisation des matrices de poids - Rétropropagation}
\label{sec-5}

On chercha à trouver les coefficients $\left\{w_{ij}^{(n)}\right\}$ qui minimise
l'erreur. On sait l'erreur $E$ diminue le plus rapidement dans la direction
donnée par l'opposé de son gradient.

Autrement dit, à chaque $w_{ij}^{(n)$ on va ajouter
$dw_{ij}^{(n)}=\delta_{i}^{(n)}y_{j}^{(n-1)}$.

\subsection{Récapitulatif des formules en vue de l'implémentation}
\label{sec-5-1}

Les formules suivantes peuvent être implémentées quasiment telles quelles:

\subsubsection{Version entrée en colonne}
\label{sec-5-1-1}

On note: $\delta^{(n)} = \begin{pmatrix}\delta_{1}^{(n)}\\
\delta_{2}^{(n)}\\\dots\\\delta_{m_{n}}^{(n)}\end{pmatrix}$, $L_{i}^{(n)}$ la
$i^{eme}$ de $W^{(n)}$ et $C_{j}^{(n)}$ sa $j^{eme}$ colonne.

\textbf{Dernière couche}

\begin{enumerate}
\item $\delta_{i}^{(N)} = \sigma^{(N)}'(x_{i}^{(N)})(t_{i}-y_{i}^{(N)})$
\item $dw_{ij}^{(N)}=\sigma^{(N)}'(x_{i}^{(N)})(t_{i}-y_{i}^{(N)})y_{j}^{(N-1)}$.
\end{enumerate}

\textbf{Couche intermédiaire}

\begin{enumerate}
\item \begin{align}
\delta_{i}^{(n)}&=\sigma^{(n)}'(x_{i}^{(n)})
\sum_{l=1}^{m_{n+1}}w_{li}^{(n+1)}\delta_{l}^{(n+1)}\\&=
\sigma^{(n)}'(x_{i}^{(n)})^{t}C_{i}^{(n+1)}.\delta^{(n+1)}
\end{align}
\item \begin{align}
dw_{ij}^{(n)}&=-\sigma^{(n)}'(x_{i}^{(n)})
\sum_{l=1}^{m_{n+1}}w_{li}^{(n+1)}\delta_{l}^{(n+1)}y_{j}^{(n-1)}\\&=
\sigma^{(n)}'(x_{i}^{(n)})^{t}C_{i}^{(n+1)}.\delta^{(n+1)}y_{j}^{(n-1)}
\end{align}
\end{enumerate}

\textbf{Première couche}

\begin{enumerate}
\item \begin{align}
\delta_{i}^{(1)}&=\sigma^{(1)}'(x_{i}^{(1)})
\sum_{l=1}^{m_{2}}w_{li}^{(2)}\delta_{l}^{(2)}\\&=
\sigma^{(1)}'(x_{i}^{(1)})^{t}C_{i}^{(2)}.\delta^{(2)}
\end{align}
\item \begin{align}
dw_{ij}^{(1)}&=-\sigma^{(1)}'(x_{i}^{(1)})
\sum_{l=1}^{m_{2}}w_{li}^{(2)}\delta_{l}^{(2)}x_{j}^{(0)}\\&=
\sigma^{(1)}'(x_{i}^{(1)})^{t}C_{i}^{(2)}.\delta^{(2)}x_{j}^{(0)}
\end{align}
\end{enumerate}

\subsubsection{Version transposée}
\label{sec-5-1-2}

Il peut être préférable de considéer l'entrée et les différentes couches du
réseau comme des lignes plutôt que comme des colonnes. Il suffit pour cela de
transposer toutes les matrices dans ce qui a été fait précedemment. On note alors
que le produit à gauche devient un produit à droite, pour passer d'une couche à la
suivante.

Les formules ci-dessus deviennent alors:

\textbf{Dernière couche}

\begin{enumerate}
\item $\delta_{j}^{(N)} = \sigma^{(N)}'(x_{j}^{(N)})(t_{j}-y_{j}^{(N)})$
\item $dw_{ij}^{(N)}=\sigma^{(N)}'(x_{j}^{(N)})(t_{j}-y_{j}^{(N)})y_{i}^{(N-1)}$.
\end{enumerate}

\textbf{Couche intermédiaire}

\begin{enumerate}
\item $\delta_{j}^{(n)}=\sigma^{(n)}'(x_{j}^{(n)})
  \sum_{l=1}^{p_{n+1}}w_{jl}^{(n+1)}\delta_{l}^{(n+1)}=
  \sigma^{(n)}'(x_{j}^{(n)})\delta^{(n)}.^{t}L_{j}^{(n)}$
\item $dw_{ij}^{(n)}=\sigma^{(n)}'(x_{j}^{(n)})
  \sum_{l=1}^{p_{n+1}}w_{jl}^{(n+1)}^{t}\delta_{l}^{(n+1)}y_{i}^{(n-1)}=
  \sigma^{(n)}'(x_{j}^{(n)})\delta^{(n+1)}.^{t}L_{j}^{(n+1)}y_{i}^{(n-1)}$.
\end{enumerate}

\textbf{Première couche}

\begin{enumerate}
\item $\delta_{j}^{(1)}=\sigma^{(1)}'(x_{j}^{(1)})
  \sum_{l=1}^{p_{2}}w_{jl}^{(21)}\delta_{l}^{(2)}=
  \sigma^{(1)}'(x_{i}^{(1)})\delta^{(1)}.^{t}L_{j}^{(1)}$
\item $dw_{ij}^{(1)}=\sigma^{(1)}'(x_{j}^{(1)})
  \sum_{l=1}^{p_{2}}w_{jl}^{(2)}\delta_{l}^{(2)}x_{i}^{(0)}=
  \sigma^{(1)}'(x_{i}^{(1)})\delta^{(2)}.^{t}L_{j}^{(2)}x_{i}^{(0)}$.
\end{enumerate}

On peut exprimer cela matriciellement:

\textbf{Dernière couche}

\begin{enumerate}
\item $\delta^{(N)}=\sigma^{(N)}'(X^{(N)})\times(T-Y^{(N)})$
\item $dw^{(N)}=^{t}Y^{(N-1)}.\delta^{(N)}$
\end{enumerate}

\textbf{Couche intermédiaire}

\begin{enumerate}
\item $\delta^{(n)}=\sigma^{(n)}'(X^{(N)})\times (\delta^{(n+1)}.^{t}W^{(n+1)})$
\item $dw^{(n)}=^{t}Y^{(n-1)}.\delta^{(n)}$
\end{enumerate}

\textbf{Première couche}

\begin{enumerate}
\item $\delta^{(1)}=\sigma^{(1)}'(X^{(N)})\times (\delta^{(2)}.^{t}W^{(2)})$
\item $dw^{(1)}=^{t}X^{(0)}.\delta^{(1)}$
\end{enumerate}

\subsubsection{Remarques}
\label{sec-5-1-3}

Le vecteur $\delta^{(n)}$ est l'opposé du gradient de l'erreur $E$, lorsque cette
dernière est exprimée en fonction des $x_{i}^{(n)}$. Autrement dit:

\begin{equation}
\delta^{(n)} = - \nabla \left(E(x_{1}^{(n)},\dots,x_{m_{n}}^{(n)})\right)
\end{equation}


\subsection{Sigmoïdes - Fonctions d'activation usuelles}
\label{sec-5-2}

On considère la fonction $f$ définie sur $\mathbb{R}$ par
$f(x)=\frac{1}{1+e^{-ax}}$, où $a$ est un réel strictement positif.

$f$ est de classe $C^{\infty}$ et on a:

\begin{align}
f'(x) & = -\frac{-ae^{-ax}}{(1+e^{-ax})^{(2)}}\\
& = a\frac{e^{-ax}}{(1+e^{-ax})^{(2)}}
\end{align}

On constate que $f$ est solution de l'équation différentielle $y'=ay(1-y)$. En
effet:

\begin{align}
f(x)(1-f(x)) & = \frac{1}{1+e^{-ax}}(1-\frac{1}{1+e^{-ax}})\\
& = \frac{1}{1+e^{-ax}}(\frac{1+e^{-ax}-1}{1+e^{-ax}})\\
& = \frac{e^{-ax}}{(1+e^{-ax})^{(2)}}\\
& = \frac{1}{a}f'(x)
\end{align}

\section{Exemple}
\label{sec-6}

\subsection{Les mathématiques}
\label{sec-6-1}

On va utiliser la version transposée avec le cas suivant:
\begin{itemize}
\item $N=2$
\item $\sigma^{n}=\sigma$
\item $(n_{1}, n_{0})=(3,2)$ donc $W^{(1)}\in\mathcal{M}_{2,3}(\mathbb{R})$
\item $(n_{2}, n_{1})=(1,3)$ donc $W^{(2)}\in\mathcal{M}_{3,1}(\mathbb{R})$
\item $X_{0}$ prendra successivement pour valeurs:
\begin{itemize}
\item $(0,0)$
\item $(0,1)$
\item $(1,0)$
\item $(1,1)$
\end{itemize}
\item $T$ prendra successivement pour valeurs:
\begin{itemize}
\item $0$
\item $1$
\item $1$
\item $0$
\end{itemize}
\end{itemize}

On obtient:

\begin{enumerate}
\item $\delta^{(2)}=\sigma^{(2)}'(X^{(2)})\times(T-Y^{(2)})$
\item $\delta^{(1)}=\sigma^{(1)}'(X^{(2)})\times (\delta^{(2)}.^{t}W^{(2)})$
\end{enumerate}


\begin{enumerate}
\item $dw^{(2)}=-^{t}Y^{(1)}.\delta^{(2)}$
\item $dw^{(1)}=-^{t}X^{(0)}.\delta^{(1)}$
\end{enumerate}


\textbf{Remarque:}

\begin{align}
\sigma'(X^{(n)}) & = \sigma(X^{(n)})(1-\sigma(X^{(n)}))\\
                     & = Y^{(n)}(1-Y^{(n)})\\
                     & = \sigma\_(Y^{(n)})
\end{align}

\subsection{Implémentations en python}
\label{sec-6-2}

\subsubsection{Version basique}
\label{sec-6-2-1}

\begin{verbatim}
# coding: utf-8
# XOR basique
import numpy as np

iterations = 6000                # Nombre d'itérations

tailleX0, tailleX1, tailleX2 = 2, 3, 1

X0 = np.array([[0,0], [0,1], [1,0], [1,1]])
T = np.array([ [0],   [1],   [1],   [0]])

def sigmoide (x):
    return 1/(1 + np.exp(-x))    # fonction d'activation
def sigmoide_(x):
    return x * (1 - x)           # dérivée de la fonction d'activation

# Poids
W1 = np.random.uniform(size=(tailleX0, tailleX1))
W2 = np.random.uniform(size=(tailleX1,tailleX2))

for i in range(iterations):

    X1 = np.dot(X0, W1)                 # entrée couche 1
    Y1 = sigmoide(X1)                   # activation couche 1
    X2 = np.dot(Y1, W2)                 # entrée couche 2
    Y2 = sigmoide(X2)                   # activation couche 2

    E = T - Y2                          # erreur

    d2 = sigmoide_(Y2) * E              # d2  
    dW2 = Y1.T.dot(d2)                  # somme sur les entrées des dW1
    W2 += dW2                           # mise à jour des poides de la couche 2

    d1 = sigmoide_(Y1) * d2.dot(W2.T)   # d1 
    dW1 = X0.T.dot(d1)                  # somme sur les entrées des dW2
    W1 += dW1                           # et des poids de la couche 1

print(Y2)
\end{verbatim}

\subsubsection{Version améliorée}
\label{sec-6-2-2}

Le but de l'amélioration qui va suivre est d'écrire des fonctions réutilisables
pour résoudre des problèmes plus compliqués.

\section{Améliorations possibles}
\label{sec-7}

\subsection{Taux d'apprentissage}
\label{sec-7-1}

\subsection{Biais}
\label{sec-7-2}

Introduire un biais signifie transformer l'écriture
$Y^{(n)}=\sigma^{n}(X^{(n)})$ en $Y^{(n)}=\sigma^{n}(X^{(n)}+B^{(n)})$.

A nouveau, calculons, en notations colonnes, les dérivées partielles de l'erreur
en fonctions des biais de la dernière couche ($n=N$):

\begin{align}
\frac{\partial E}{\partial b_{i}^{(N)}}
& = \sum_{k=1}^{m_{N}}\frac{\partial E}{\partial x_{k}^{(N)}}
\frac{\partial x_{k}^{(N)}}{\partial b_{i}^{(N)}}\\
& = \sum_{k=1}^{m_{N}}\frac{\partial E}{\partial x_{k}^{(N)}}
\frac{\partial (\sum_{l=1}^{n_{N-1}}w_{il}^{(N)}y_{l}^{(N-1)}+b_{l}^{(N)})}
{\partial b_{i}^{(N)}}\\
& = \sum_{k=1}^{m_{N}}\frac{\partial E}{\partial x_{k}^{(N)}}.1\\
& = -\sum_{k=1}^{m_{N}}\delta_{k}^{N}
\end{align}

L'expression ci-dessus ne dépend pas de $i$.

Pour une couche quelconque:

\begin{align}
\frac{\partial E}{\partial b_{i}^{(n)}}
& = \sum_{k=1}^{m_{n+1}}\frac{\partial E}{\partial x_{k}^{(n)}}
\frac{\partial x_{l}^{(n)}}{\partial b_{i}^{(n)}}\\
& = \sum_{k=1}^{m_{n+1}}\frac{\partial E}{\partial x_{k}^{(n)}}
\frac{\partial \left(\sum w_{kl}^{(n-1)}y_{l}^{(n-1)}+b_{l}^{(n)}\right)}{\partial b_{i}^{(n)}}\\
& = \sum_{k=1}^{m_{n+1}}\frac{\partial E}{\partial x_{k}^{(n)}}.1\\
& = -\sum_{k=1}^{m_{n+1}}\delta_{k}^{(n)}
\end{align}

\section{Exercices}
\label{sec-8}

\subsection{Quadrants}
\label{sec-8-1}

Implémenter un réseau de neurones capable de déterminer si un point généré
aléatoirement se situe dans le premier, le deuxième, le troisième ou le
quatrième quadrant du plan.

Le reseau doit produire les sorties suivantes:

\begin{itemize}
\item\relax [1,0,0,0] si le point est dans le premier quadrant ($x>0$ et $y>0$)
\item\relax [0,1,0,0] si le point est dans le deuxième quadrant ($x<0$ et $y>0$)
\item\relax [0,0,1,0] si le point est dans le troisième quadrant ($x<0$ et $y<0$)
\item\relax [0,0,0,1] si le point est dans le quatrième quadrant ($x>0$ et $y<0$)
\end{itemize}

\subsection{Implémentation du biais}
\label{sec-8-2}
% Emacs 24.4.1 (Org mode 8.2.10)
\end{document}